% Load the data
data = [
    29.7242, 99.9742, 60.9402, 69.85361092;
    11.9503, 93.7646, 58.8583, 43.84814578;
    76.4815, 46.1927, 82.3581, 118.1574362;
    5.0282, 73.3823, 38.6828, 18.65339897;
   88.4862,	60.6119,	90.3234,	142.2163329;
   68.5463	11.4105	83.3835	77.34955628;
1.6262,	56.712	90.7982	80.36538178;
13.8456	67.0582	82.596	70.5056023;
51.2546	15.1518	73.7969	68.22581898;
51.8971	60.269	74.3554	75.56511829;
76.8473	19.9725	70.1846	58.60710776;
85.4598	81.9564	76.9866	129.3091413;
86.109	50.103	30.1235	53.21744479;
47.953	55.6387	28.012	50.52714725
28.4457	11.8727	78.7104	69.33054331
56.5805	72.8846	48.737	64.99142279
77.8442	98.2238	68.0984	118.8354521
85.7716	52.4878	96.7809	143.6850519
40.4215	85.2118	29.406	43.0910161
66.8475	80.734	20.7094	67.25745313
29.945	92.5594	26.5511	32.76652144
51.1983	78.788	48.0096	63.38733353
27.0658	42.5764	1.2325	13.53883383
49.5459	13.0367	92.6931	95.37925822
78.5782	83.6526	43.7158	87.84341903
78.6947	0.3227	68.7127	35.46829921
57.4227	89.3114	49.1668	71.45875951
50.0528	54.4847	87.7671	106.3017563
69.5813	3.1913	24.3609	14.15508252
7.5936	26.6748	93.2857	89.04779586
41.4621	98.8977	61.8096	71.20932979
73.4453	6.5347	40.7557	25.40970084
54.366	91.8297	35.5997	64.5975211
71.1906	89.0381	77.6264	123.6453374
6.8158	43.4178	28.2241	4.92526862
64.6542	0.6168	12.6132	1.989715248
92.8467	23.1407	69.7053	67.07366479
45.4502	69.5161	34.393	44.42399097
13.4573	70.5482	51.2765	35.78667744
73.0155	85.5291	34.1298	73.09793249
42.8423	22.0407	71.2946	49.27194271
79.843	34.0358	38.4121	45.93009806
30.3322	33.4753	66.4869	56.35887366
86.2841	23.2804	61.1398	64.46803506
65.0735	59.3083	79.4092	101.652197
39.7335	2.0231	53.5352	35.46402483
99.581	89.4285	24.3926	95.00378393
46.4925	20.6745	58.9453	42.35757583
19.5366	83.7862	24.8078	22.52324416
64.7081	28.691	35.7769	33.36526671
94.4172	94.8989	52.6635	117.3353265
58.2921	43.2484	87.663	107.0584163
26.2067	18.8646	90.8188	87.42433346
86.0304	29.7689	50.4808	48.09341543
65.8531	58.0685	1.1774	38.25377008
84.1535	19.2134	60.1928	55.40048029
75.5716	63.7445	83.9066	116.5759138
76.5583	10.533	58.0779	49.79431042
78.8061	98.3442	74.0997	138.408884
50.1839	44.9371	27.1766	25.9368652
49.2079	57.7028	39.9456	42.35084571
78.8285	0.2225	71.7728	51.68874161
17.6132	83.2778	30.1173	26.73840306
1.0596	24.0773	30.3955	9.493987273
9.3821	39.6854	83.6563	70.70708921
66.7637	42.0204	32.4457	38.58160828
24.4154	80.8968	87.2339	95.8488104
63.6883	57.3618	87.4182	117.9521722
66.4372	34.7961	43.034	41.63680611
28.2934	98.0997	3.8566	31.90447416
49.2591	25.4174	50.3261	40.8475459
19.7178	53.7725	67.5211	56.19374346
24.4612	68.7049	53.8566	47.81137663
45.0732	12.6391	56.7148	37.86253221
72.396	92.5109	92.9508	165.3727034
73.2916	90.6214	45.2071	82.85469291
23.2612	51.0561	33.4056	27.03560265
93.9723	87.5542	65.2249	124.8195713
35.8515	29.9844	60.598	48.47103321
8.1567	58.2246	95.6379	91.21528511
87.548	57.7415	55.559	76.41955323
60.416	10.5719	4.6734	15.60552578
42.9063	46.9686	47.2757	38.50240653
60.2927	59.1171	99.6936	135.0314346
19.9886	64.5351	23.3757	9.3638965
43.3199	46.0246	4.4223	20.13337807
0.3034	9.3173	4.5991	4.239785896
49.4376	63.8455	97.4435	120.5160398
49.1004	66.1735	25.0294	27.75616184
81.2109	2.3332	32.2028	7.265016
37.5395	30.9248	44.438	36.35637374
95.6554	49.439	37.7035	61.50661233
25.127	38.1497	21.8754	9.37120637
46.3016	32.7186	48.097	38.28244939
82.7952	31.1491	70.7967	72.91168695
90.719	78.7532	28.1187	91.3507284
79.6704	56.7014	13.1125	55.89360875
97.1878	2.781	6.6012	10.13855113
86.8116	73.6254	38.4751	80.71872095
14.5376	41.3158	1.2558	6.022096077
];

% Split inputs (A, B, C) and output (D)
X = data(:, 1:3)';
y = data(:, 4)';

% Normalize the inputs
[X_norm, ps] = mapminmax(X);

% Split the dataset into training (70%) and testing (30%)
trainRatio = 0.7; valRatio = 0.15; testRatio = 0.15;
[trainInd, valInd, testInd] = dividerand(size(X_norm, 2), trainRatio, valRatio, testRatio);

X_train = X_norm(:, trainInd);
y_train = y(trainInd);
X_val = X_norm(:, valInd);
y_val = y(valInd);
X_test = X_norm(:, testInd);
y_test = y(testInd);

% Create a Feedforward Neural Network with 1 hidden layer of 10 neurons
net = feedforwardnet(10, 'trainlm');
net.divideFcn = 'divideind';  % Use indices for train/test split
net.divideParam.trainInd = trainInd;
net.divideParam.valInd = valInd;
net.divideParam.testInd = testInd;

% Set training parameters
net.trainParam.epochs = 100;
net.trainParam.goal = 1e-6;

% Train the network
[net, tr] = train(net, X_norm, y);

% Evaluate the network on test data
y_pred = net(X_test);

% Plot Training/Testing Loss
figure;
plot(tr.epoch, tr.perf, 'b', 'LineWidth', 1.5); hold on;
plot(tr.epoch, tr.vperf, 'r', 'LineWidth', 1.5);
plot(tr.epoch, tr.tperf, 'g', 'LineWidth', 1.5);
xlabel('Epochs'); ylabel('Mean Squared Error');
legend('Training Loss', 'Validation Loss', 'Test Loss');
title('Training, Validation, and Test Loss');
grid on;

% Plot Actual vs Predicted Output (on Test set)
figure;
scatter(y_test, y_pred);
xlabel('Actual Outputs (D)'); ylabel('Predicted Outputs (D)');
title('Actual vs Predicted Outputs on Test Data');
grid on;

% Sensitivity Analysis
input_names = {'A', 'B', 'C'};
for i = 1:3
    % Vary one input feature, keep others fixed
    X_sensitivity = X_test;
    test_range = linspace(min(X_test(i, :)), max(X_test(i, :)), 100);
    predictions = zeros(1, 100);
    
    for j = 1:length(test_range)
        X_sensitivity(i, :) = test_range(j);
        predictions(j) = net(X_sensitivity(:, j));
    end
    
    % Plot sensitivity for the current feature
    figure;
    plot(test_range, predictions, 'LineWidth', 1.5);
    xlabel(sprintf('%s Input Values', input_names{i}));
    ylabel('Predicted Output (D)');
    title(sprintf('Sensitivity Analysis for Input %s', input_names{i}));
    grid on;
end

% Overfitting discussion:
% Compare training loss vs validation loss, if the validation loss
% increases after a certain point in the training, it may indicate overfitting.
